import pandas as pd
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
import matplotlib.pyplot as plt

# One-Hot Encoding Fonksiyonu
def one_hot_encode(data, columns):
    # Kategorik sütunları seçme
    categorical_data = data[columns]

    # One-Hot Encoder oluşturma
    encoder = OneHotEncoder(sparse_output=False, drop='first')

    # Kategorik veriyi dönüştürme
    encoded_data = encoder.fit_transform(categorical_data)

    # One-Hot Encoding sonucunu DataFrame'e dönüştürme
    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns))

    # Dönüştürülmüş veriyi orijinal veriyle birleştirme
    preprocessed_data = pd.concat([data.drop(columns, axis=1), encoded_df], axis=1)

    return preprocessed_data

# Min-Max Scaling Fonksiyonu
def min_max_scale(data, column):
    # Min-Max Scaler oluşturma
    scaler = MinMaxScaler()

    # Sütunu seçme ve ölçeklendirme
    scaled_column = scaler.fit_transform(data[[column]])

    # Ölçeklendirilmiş sütunu orijinal veri setine ekleme
    data[column] = scaled_column

    return data

# NaN Değerleri Ortalama ile Doldurma Fonksiyonu
def fill_nan_with_mean(data):
    for column in data.columns:
        if data[column].isnull().any():
            mean = data[column].mean()
            data[column].fillna(mean, inplace=True)
    return data

# Veri Ön İşleme Fonksiyonu
def veriOnIsleme(data, colomnsForOneHotEncoder, colomnsForMinMaxScaler):
    dataForOneHot = one_hot_encode(data, colomnsForOneHotEncoder)
    for column in colomnsForMinMaxScaler:
        dataForOneHot = min_max_scale(dataForOneHot, column)
    result = fill_nan_with_mean(dataForOneHot)
    return result

# Veri Setini Yükleme ve Ön İşleme
veriSetim = pd.read_csv("/content/Churn_Modelling.csv")
veriOnIslemeliSet = veriOnIsleme(veriSetim, ["Surname", "Geography", "Gender"], 
                                 ["CreditScore", "Age", "Tenure", "Balance", "NumOfProducts", "EstimatedSalary"])

# Gereksiz Sütunları Çıkarma
veriOnIslemeliSet = veriOnIslemeliSet.drop(["RowNumber", "CustomerId"], axis=1)

# Bağımsız ve Bağımlı Değişkenleri Ayırma
MyX = veriOnIslemeliSet.drop(["Exited"], axis=1)
MyY = veriOnIslemeliSet["Exited"]

# Eğitim ve Test Verilerini Ayırma
def parcalamaIslemi(A, B):
    X_train, X_test, Y_train, Y_test = train_test_split(A, B, test_size=0.2, random_state=42)
    return X_train, X_test, Y_train, Y_test

MyX_train, MyX_test, MyY_train, MyY_test = parcalamaIslemi(MyX, MyY)

# Sinir Ağı Modelini Oluşturma
model = tf.keras.Sequential([
    tf.keras.layers.Dense(150, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(100, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(100, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Modeli Derleme
model.compile(loss=tf.keras.losses.binary_crossentropy,
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=["accuracy"])

# Modeli Eğitme
history_model = model.fit(MyX_train, MyY_train, epochs=20)

# Modeli Değerlendirme
model.evaluate(MyX_test, MyY_test)

# Tahmin Yapma
y_pred = model.predict(MyX_test)
print(y_pred)
print(MyY_test)

# Kayıp Grafiğini Çizdirme
pd.DataFrame(history_model.history).plot()
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.title("Model Loss Curve")
plt.show()

# Korelasyon Analizi
print(veriOnIslemeliSet.corr()['Exited'].sort_values())
