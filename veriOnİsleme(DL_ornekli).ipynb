{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6ifHAqkHKqS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def one_hot_encode(data, columns):\n",
        "    # Kategorik sütunları seçme\n",
        "    categorical_data = data[columns]\n",
        "\n",
        "    # One-Hot Encoder oluşturma\n",
        "    encoder = OneHotEncoder(sparse=False, drop='first')\n",
        "\n",
        "    # Kategorik veriyi dönüştürme\n",
        "    encoded_data = encoder.fit_transform(categorical_data)\n",
        "\n",
        "    # One-Hot Encoding sonucunu DataFrame'e dönüştürme\n",
        "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns))\n",
        "\n",
        "    # Dönüştürülmüş veriyi orijinal veriyle birleştirme\n",
        "    preprocessed_data = pd.concat([data.drop(columns, axis=1), encoded_df], axis=1)\n",
        "\n",
        "    return preprocessed_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def min_max_scale(data, column):\n",
        "    # Min-Max Scaler oluşturma\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Sütunu seçme ve ölçeklendirme\n",
        "    scaled_column = scaler.fit_transform(data[[column]])\n",
        "\n",
        "    # Ölçeklendirilmiş sütunu yeni bir DataFrame'e dönüştürme\n",
        "    scaled_data = pd.DataFrame(scaled_column, columns=[column])\n",
        "\n",
        "    # Ölçeklendirilmiş sütunu orijinal veri setine ekleme\n",
        "    data[column] = scaled_data[column]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def fill_nan_with_mean(data):\n",
        "    for column in data.columns:\n",
        "        if data[column].isnull().any():\n",
        "            mean = data[column].mean()\n",
        "            data[column].fillna(mean, inplace=True)\n",
        "    return data\n",
        "\n",
        "\n",
        "def veriOnIsleme(data,colomnsForOneHotEncoder,colomnsForMinMaxScaler):\n",
        "  dataForOneHot=one_hot_encode(data,colomnsForOneHotEncoder)\n",
        "  for column in colomnsForMinMaxScaler:\n",
        "    dataForMinMaxScaler = min_max_scale(dataForOneHot, column)\n",
        "  result=fill_nan_with_mean(dataForMinMaxScaler)\n",
        "  return result\n",
        "\n",
        "veriSetim=pd.read_csv(\"/content/Churn_Modelling.csv\")\n",
        "veriOnİslemeliSet=veriOnIsleme(veriSetim,[\"Surname\",\"Geography\",\"Gender\"],[\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"NumOfProducts\",\"EstimatedSalary\"])\n",
        "veriOnİslemeliSet=veriOnİslemeliSet.drop([\"RowNumber\",\"CustomerId\"],axis=1)\n",
        "\n",
        "MyX=veriOnİslemeliSet.drop([\"Exited\"],axis=1)\n",
        "MyY=veriOnİslemeliSet[\"Exited\"]\n",
        "\n",
        "def parcalamaIslemi(A,B):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(A, B,test_size=0.2,random_state=42)\n",
        "  return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "MyX_train, MyX_test, MyY_train, MyY_test=parcalamaIslemi(MyX,MyY)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# BUİLDİNG A NEURAL NETWORK\n",
        "\n",
        "# set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# add layer\n",
        "\n",
        "model=tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(150),\n",
        "    tf.keras.layers.Dropout(.2),\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dropout(.2),\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(1)\n",
        "]\n",
        "\n",
        ")\n",
        "\n",
        "#compiling\n",
        "\n",
        "model.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics=[\"mae\"]\n",
        "              )\n",
        "\n",
        "# fitting\n",
        "\n",
        "history_model=model.fit(MyX_train,MyY_train,epochs=20)\n",
        "\n",
        "# evaluating\n",
        "model.evaluate(MyX_test,MyY_test)\n",
        "\n",
        "#prediction\n",
        "import matplotlib.pyplot as plt\n",
        "y_pred=model.predict(MyX_test)\n",
        "print(y_pred)\n",
        "print(MyY_test)\n",
        "#Plot the history(LOSS GRAPH)\n",
        "pd.DataFrame(history_model.history).plot()\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epochs\")\n",
        "\n",
        "pd.DataFrame(history_model.history).plot()\n",
        "plt.title(\"model_iris Loss Curve\")\n",
        "\n",
        "veriSetim.corr()['Exited'].sort_values()"
      ]
    }
  ]
}